import time
import os
import json
from collections import Counter, deque
from datetime import datetime
from io import BytesIO

import cv2
import numpy as np
import requests
import torch
from deepface import DeepFace

# MediaPipe import (ì„ íƒì )
MEDIAPIPE_AVAILABLE = False
mp = None

try:
    import mediapipe as mp
    from mediapipe.tasks import python
    from mediapipe.tasks.python import vision
    MEDIAPIPE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ MediaPipeë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("   ì„¤ì¹˜ ë°©ë²•: pip install mediapipe")
except Exception as e:
    print(f"âš ï¸ MediaPipe ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")


# =========================
# ì„¤ì •
# =========================

# YOLOv5 ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
# - ì¸í„°ë„·ì´ ê°€ëŠ¥í•˜ë©´: "yolov5s" (torch.hubì—ì„œ ë‹¤ìš´ë¡œë“œ)
# - ë¡œì»¬ yolov5 ë ˆí¬ë¥¼ ì“°ê³  ì‹¶ìœ¼ë©´: torch.hub.load("yolov5", "custom", path="yolov5s.pt")
YOLO_MODEL_NAME = "yolov5s"

# Django ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ (ë¡œì»¬ ê°œë°œ ê¸°ì¤€)
DJANGO_BASE_URL = "http://127.0.0.1:8000"
WELLBEING_API_URL = f"{DJANGO_BASE_URL}/api_root/WellbeingLog/"
POST_API_URL = f"{DJANGO_BASE_URL}/api_root/Post/"
REGISTER_API_URL = f"{DJANGO_BASE_URL}/api_root/register/"
LOGIN_API_URL = f"{DJANGO_BASE_URL}/api-token-auth/"

# ë¡œê·¸ì¸ ì •ë³´ ì €ì¥ ê²½ë¡œ (localhost)
AUTH_INFO_FILE = os.path.join(os.path.dirname(__file__), "auth_info.json")

# Post ê²Œì‹œ ì„¤ì •
ENABLE_POST_API = True  # Post API ì‚¬ìš© ì—¬ë¶€
POST_INTERVAL = 300  # Post ê²Œì‹œ ì£¼ê¸° (ì´ˆ, ê¸°ë³¸ 5ë¶„)
POST_ON_NEW_OBJECT = True  # ìƒˆ ê°ì²´ê°€ ê²€ì¶œë˜ë©´ ì¦‰ì‹œ ê²Œì‹œ

# ìš”ì•½ ì „ì†¡ ì£¼ê¸° (ì´ˆ)
SUMMARY_INTERVAL = 60  # 60ì´ˆë§ˆë‹¤ í•œ ë²ˆ ì„œë²„ë¡œ ìš”ì•½ ì „ì†¡

# ìœˆë„ìš° í¬ê¸° (ìµœê·¼ Nì´ˆ ë°ì´í„°ë§Œ ì§‘ê³„)
WINDOW_SECONDS = 60
FRAME_FPS_ASSUMPTION = 2  # ì´ˆë‹¹ 2í”„ë ˆì„ ì •ë„ë¡œ ë³¸ë‹¤ê³  ê°€ì •


def load_yolo_model():
    """
    YOLOv5 ëª¨ë¸ ë¡œë“œ (MS COCO 80ê°€ì§€ ê°ì²´ ê²€ì¶œ).
    torch.hubì„ ì‚¬ìš©í•˜ë©°, ìµœì´ˆ 1íšŒëŠ” ì¸í„°ë„·ì—ì„œ ëª¨ë¸ì„ ë‚´ë ¤ë°›ìŠµë‹ˆë‹¤.
    """
    print("YOLOv5 ëª¨ë¸ ë¡œë”© ì¤‘... (MS COCO 80ê°€ì§€ ê°ì²´ ê²€ì¶œ ê°€ëŠ¥)")
    model = torch.hub.load("ultralytics/yolov5", YOLO_MODEL_NAME, pretrained=True)
    model.conf = 0.4  # confidence threshold
    # model.classes = [0]  # ì£¼ì„ ì²˜ë¦¬: ëª¨ë“  80ê°€ì§€ ê°ì²´ ê²€ì¶œ ê°€ëŠ¥
    # COCO í´ë˜ìŠ¤: person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, 
    # traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, 
    # sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, 
    # suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, 
    # skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, 
    # bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, 
    # cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, 
    # remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, 
    # clock, vase, scissors, teddy bear, hair drier, toothbrush
    return model


def estimate_movement(prev_boxes, curr_boxes):
    """
    ê°„ë‹¨í•œ í™œë™ì„± ì§€í‘œ:
    ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ ì‚¬ëŒ bbox ì¤‘ì‹¬ ì´ë™ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if not prev_boxes or not curr_boxes:
        return 0.0

    prev_centers = np.array(
        [((x1 + x2) / 2.0, (y1 + y2) / 2.0) for x1, y1, x2, y2 in prev_boxes]
    )
    curr_centers = np.array(
        [((x1 + x2) / 2.0, (y1 + y2) / 2.0) for x1, y1, x2, y2 in curr_boxes]
    )

    n = min(len(prev_centers), len(curr_centers))
    if n == 0:
        return 0.0

    prev_centers = prev_centers[:n]
    curr_centers = curr_centers[:n]

    diffs = np.linalg.norm(curr_centers - prev_centers, axis=1)
    return float(np.mean(diffs))


def analyze_emotion(face_img):
    """
    DeepFaceë¥¼ ì‚¬ìš©í•´ ì–¼êµ´ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        result = DeepFace.analyze(
            face_img, actions=["emotion"], enforce_detection=False
        )
        if isinstance(result, list):
            result = result[0]
        return result.get("dominant_emotion")
    except Exception as e:
        print("Emotion analyze error:", e)
        return None


def estimate_pose(pose_landmarks, image_height, image_width):
    """
    MediaPipe Poseë¥¼ ì‚¬ìš©í•˜ì—¬ ìì„¸ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
    MediaPipe 0.10+ tasks API ì‚¬ìš©
    Returns: 'sitting', 'standing', 'bending', or None
    """
    if not pose_landmarks or len(pose_landmarks) == 0:
        return None
    
    try:
        # ì£¼ìš” ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
        LEFT_SHOULDER = 11
        RIGHT_SHOULDER = 12
        LEFT_HIP = 23
        RIGHT_HIP = 24
        LEFT_KNEE = 25
        RIGHT_KNEE = 26
        LEFT_ANKLE = 27
        RIGHT_ANKLE = 28
        
        # ëœë“œë§ˆí¬ ì¢Œí‘œ ì¶”ì¶œ (tasks APIëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        landmarks = pose_landmarks
        
        def get_landmark(idx):
            if idx >= len(landmarks):
                return None
            lm = landmarks[idx]
            # tasks APIëŠ” x, y, z ì†ì„±ì„ ê°€ì§
            return (lm.x * image_width, lm.y * image_height)
        
        # ì–´ê¹¨ì™€ ì—‰ë©ì´ ì¤‘ì‹¬ì 
        left_shoulder = get_landmark(LEFT_SHOULDER)
        right_shoulder = get_landmark(RIGHT_SHOULDER)
        left_hip = get_landmark(LEFT_HIP)
        right_hip = get_landmark(RIGHT_HIP)
        left_knee = get_landmark(LEFT_KNEE)
        right_knee = get_landmark(RIGHT_KNEE)
        left_ankle = get_landmark(LEFT_ANKLE)
        right_ankle = get_landmark(RIGHT_ANKLE)
        
        # í•„ìˆ˜ ëœë“œë§ˆí¬ í™•ì¸
        if not all([left_shoulder, right_shoulder, left_hip, right_hip, 
                   left_knee, right_knee, left_ankle, right_ankle]):
            return None
        
        shoulder_center_y = (left_shoulder[1] + right_shoulder[1]) / 2
        hip_center_y = (left_hip[1] + right_hip[1]) / 2
        knee_center_y = (left_knee[1] + right_knee[1]) / 2
        ankle_center_y = (left_ankle[1] + right_ankle[1]) / 2
        
        # ì–´ê¹¨ì™€ ì—‰ë©ì´ ì‚¬ì´ ê±°ë¦¬
        torso_height = abs(hip_center_y - shoulder_center_y)
        
        # ë°œëª©ì´ ë¬´ë¦ë³´ë‹¤ ì•„ë˜ì— ìˆëŠ”ì§€ í™•ì¸ (ì„œ ìˆëŠ”ì§€)
        is_standing = ankle_center_y > knee_center_y
        
        # ë¬´ë¦ê³¼ ë°œëª©ì˜ ë†’ì´ ì°¨ì´ (ë‹¤ë¦¬ê°€ í´ì ¸ ìˆëŠ”ì§€)
        leg_extended = (ankle_center_y - knee_center_y) > (knee_center_y - hip_center_y) * 0.5
        
        # ìƒì²´ê°€ ì•ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆëŠ”ì§€ (ìˆ™ì—¬ ìˆëŠ”ì§€)
        # ì–´ê¹¨ì™€ ì—‰ë©ì´ì˜ ìˆ˜ì§ ê±°ë¦¬ì™€ ìˆ˜í‰ ê±°ë¦¬ë¥¼ ë¹„êµ
        shoulder_hip_vertical = abs(hip_center_y - shoulder_center_y)
        shoulder_hip_horizontal = abs((left_hip[0] + right_hip[0]) / 2 - (left_shoulder[0] + right_shoulder[0]) / 2)
        
        # ê°ë„ ê³„ì‚° (ìˆ˜í‰ ê±°ë¦¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë‚˜ëˆ„ê¸° ì˜¤ë¥˜ ë°©ì§€)
        if shoulder_hip_horizontal < 1:
            shoulder_hip_horizontal = 1
        
        # ìƒì²´ ê¸°ìš¸ê¸° ê°ë„ (0ë„ = ì™„ì „íˆ ë˜‘ë°”ë¦„, 90ë„ = ì™„ì „íˆ ìˆ˜í‰)
        tilt_angle = np.arctan2(shoulder_hip_vertical, shoulder_hip_horizontal) * 180 / np.pi
        
        # ìƒì²´ê°€ ì•ìœ¼ë¡œ ê¸°ìš¸ì–´ì¡ŒëŠ”ì§€ (ì–´ê¹¨ê°€ ì—‰ë©ì´ë³´ë‹¤ ì•ì— ìˆëŠ”ì§€)
        shoulder_forward = (left_shoulder[0] + right_shoulder[0]) / 2 < (left_hip[0] + right_hip[0]) / 2
        
        # ìì„¸ íŒë‹¨
        if not is_standing or not leg_extended:
            # ë‹¤ë¦¬ê°€ í´ì ¸ ìˆì§€ ì•Šê±°ë‚˜ ë°œëª©ì´ ë¬´ë¦ ìœ„ì— ìˆìœ¼ë©´ ì•‰ì•„ ìˆìŒ
            return 'sitting'
        elif tilt_angle < 60 and shoulder_forward:
            # ìƒì²´ê°€ ê¸°ìš¸ì–´ì§€ê³  ì•ìœ¼ë¡œ ìˆ™ì—¬ì¡Œìœ¼ë©´ bending
            # (60ë„ ë¯¸ë§Œì´ë©´ ìƒì²´ê°€ ë§ì´ ê¸°ìš¸ì–´ì§)
            return 'bending'
        else:
            # ê·¸ ì™¸ëŠ” ì„œ ìˆìŒ
            return 'standing'
    except Exception as e:
        print(f"Pose estimation error: {e}")
        return None


def estimate_head_pose(face_landmarks, image_width, image_height):
    """
    MediaPipe Face Meshë¥¼ ì‚¬ìš©í•˜ì—¬ Head Poseë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
    MediaPipe 0.10+ tasks API ì‚¬ìš©
    Returns: {"pitch": float, "yaw": float, "roll": float} or None
    """
    if not face_landmarks or len(face_landmarks) == 0:
        return None
    
    try:
        # ì–¼êµ´ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (MediaPipe Face Mesh)
        # ì½” ë, í„±, ì™¼ìª½ ê·€, ì˜¤ë¥¸ìª½ ê·€, ì™¼ìª½ ëˆˆ, ì˜¤ë¥¸ìª½ ëˆˆ
        NOSE_TIP = 1
        CHIN = 175
        LEFT_EAR = 234
        RIGHT_EAR = 454
        LEFT_EYE = 33
        RIGHT_EYE = 263
        
        # tasks APIëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        landmarks = face_landmarks
        
        def get_landmark_3d(idx):
            if idx >= len(landmarks):
                return None
            lm = landmarks[idx]
            # tasks APIëŠ” x, y, z ì†ì„±ì„ ê°€ì§
            return np.array([lm.x * image_width, lm.y * image_height, lm.z * image_width])
        
        # 3D ì¢Œí‘œ ì¶”ì¶œ
        nose_tip = get_landmark_3d(NOSE_TIP)
        chin = get_landmark_3d(CHIN)
        left_ear = get_landmark_3d(LEFT_EAR)
        right_ear = get_landmark_3d(RIGHT_EAR)
        left_eye = get_landmark_3d(LEFT_EYE)
        right_eye = get_landmark_3d(RIGHT_EYE)
        
        # ì–¼êµ´ ì¤‘ì‹¬ì„  (ì½”-í„±)
        face_center = (nose_tip + chin) / 2
        
        # ì¢Œìš° ê·€ ì¤‘ì‹¬
        ear_center = (left_ear + right_ear) / 2
        
        # ëˆˆ ì¤‘ì‹¬
        eye_center = (left_eye + right_eye) / 2
        
        # Pitch (ìƒí•˜ ì›€ì§ì„): ì–¼êµ´ ì¤‘ì‹¬ì„ ê³¼ ìˆ˜ì§ì„ ì˜ ê°ë„
        face_vector = chin - nose_tip
        pitch = np.arctan2(face_vector[1], face_vector[2]) * 180 / np.pi
        
        # Yaw (ì¢Œìš° ì›€ì§ì„): ê·€ ì¤‘ì‹¬ì„ ê³¼ ìˆ˜í‰ì„ ì˜ ê°ë„
        ear_vector = right_ear - left_ear
        yaw = np.arctan2(ear_vector[0], ear_vector[2]) * 180 / np.pi
        
        # Roll (íšŒì „): ëˆˆ ì¤‘ì‹¬ì„ ê³¼ ìˆ˜í‰ì„ ì˜ ê°ë„
        eye_vector = right_eye - left_eye
        roll = np.arctan2(eye_vector[1], eye_vector[0]) * 180 / np.pi
        
        return {
            "pitch": float(pitch),
            "yaw": float(yaw),
            "roll": float(roll)
        }
    except Exception as e:
        print(f"Head pose estimation error: {e}")
        return None


def analyze_eye_blink(face_landmarks, prev_eye_state, frame_count):
    """
    MediaPipe Face Meshë¥¼ ì‚¬ìš©í•˜ì—¬ ëˆˆ ê¹œë¹¡ì„ì„ ê°ì§€í•˜ê³  ì§‘ì¤‘ë„/í”¼ë¡œë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    MediaPipe 0.10+ tasks API ì‚¬ìš©
    Returns: (blink_count, focus_level, fatigue_level, new_eye_state)
    """
    if not face_landmarks or len(face_landmarks) == 0:
        return 0, 0.0, 0.0, prev_eye_state
    
    try:
        # ëˆˆ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (MediaPipe Face Mesh)
        # ì™¼ìª½ ëˆˆ: ìƒë‹¨ 159, í•˜ë‹¨ 145, ì¢Œì¸¡ 33, ìš°ì¸¡ 133
        # ì˜¤ë¥¸ìª½ ëˆˆ: ìƒë‹¨ 386, í•˜ë‹¨ 374, ì¢Œì¸¡ 362, ìš°ì¸¡ 263
        LEFT_EYE_TOP = 159
        LEFT_EYE_BOTTOM = 145
        LEFT_EYE_LEFT = 33
        LEFT_EYE_RIGHT = 133
        
        RIGHT_EYE_TOP = 386
        RIGHT_EYE_BOTTOM = 374
        RIGHT_EYE_LEFT = 362
        RIGHT_EYE_RIGHT = 263
        
        # tasks APIëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        landmarks = face_landmarks
        
        def get_landmark(idx):
            if idx >= len(landmarks):
                return None
            lm = landmarks[idx]
            # tasks APIëŠ” x, y ì†ì„±ì„ ê°€ì§
            return (lm.x, lm.y)
        
        # ì™¼ìª½ ëˆˆ ì¢Œí‘œ
        left_eye_top = get_landmark(LEFT_EYE_TOP)
        left_eye_bottom = get_landmark(LEFT_EYE_BOTTOM)
        left_eye_left = get_landmark(LEFT_EYE_LEFT)
        left_eye_right = get_landmark(LEFT_EYE_RIGHT)
        
        # ì˜¤ë¥¸ìª½ ëˆˆ ì¢Œí‘œ
        right_eye_top = get_landmark(RIGHT_EYE_TOP)
        right_eye_bottom = get_landmark(RIGHT_EYE_BOTTOM)
        right_eye_left = get_landmark(RIGHT_EYE_LEFT)
        right_eye_right = get_landmark(RIGHT_EYE_RIGHT)
        
        # ëˆˆì˜ ë†’ì´ì™€ ë„ˆë¹„ ê³„ì‚°
        left_eye_height = abs(left_eye_top[1] - left_eye_bottom[1])
        left_eye_width = abs(left_eye_right[0] - left_eye_left[0])
        right_eye_height = abs(right_eye_top[1] - right_eye_bottom[1])
        right_eye_width = abs(right_eye_right[0] - right_eye_left[0])
        
        # ëˆˆ ì¢…íš¡ë¹„ (EAR: Eye Aspect Ratio)
        left_ear = left_eye_height / (left_eye_width + 1e-6)
        right_ear = right_eye_height / (right_eye_width + 1e-6)
        avg_ear = (left_ear + right_ear) / 2
        
        # ëˆˆ ê¹œë¹¡ì„ ê°ì§€ (EARì´ ì„ê³„ê°’ ì´í•˜ì¼ ë•Œ)
        EAR_THRESHOLD = 0.25
        is_blinking = avg_ear < EAR_THRESHOLD
        
        # ì´ì „ ìƒíƒœì™€ ë¹„êµí•˜ì—¬ ê¹œë¹¡ì„ ì¹´ìš´íŠ¸
        blink_count = 0
        if prev_eye_state is not None:
            if not prev_eye_state['was_blinking'] and is_blinking:
                blink_count = 1
        
        # ì§‘ì¤‘ë„ ê³„ì‚° (ëˆˆì´ ì—´ë ¤ìˆê³  ì•ˆì •ì ì¼ ë•Œ ë†’ìŒ)
        # EARì´ ì •ìƒ ë²”ìœ„(0.25~0.35)ì— ìˆê³  ì•ˆì •ì ì´ë©´ ì§‘ì¤‘ë„ ë†’ìŒ
        if 0.25 <= avg_ear <= 0.35:
            focus_level = min(1.0, avg_ear / 0.3)
        else:
            focus_level = max(0.0, 1.0 - abs(avg_ear - 0.3) * 2)
        
        # í”¼ë¡œë„ ê³„ì‚° (ëˆˆ ê¹œë¹¡ì„ ë¹ˆë„ê°€ ë‚®ê±°ë‚˜ ëˆˆì´ ìì£¼ ê°ê¸°ë©´ í”¼ë¡œë„ ë†’ìŒ)
        # í”„ë ˆì„ë‹¹ ê¹œë¹¡ì„ ë¹ˆë„ê°€ ë‚®ìœ¼ë©´ í”¼ë¡œë„ ì¦ê°€
        if prev_eye_state:
            time_since_last_blink = frame_count - prev_eye_state.get('last_blink_frame', 0)
            # 3ì´ˆ ì´ìƒ ê¹œë¹¡ì„ì´ ì—†ìœ¼ë©´ í”¼ë¡œë„ ì¦ê°€
            if time_since_last_blink > 90:  # ì•½ 3ì´ˆ (30fps ê¸°ì¤€)
                fatigue_level = min(1.0, (time_since_last_blink - 90) / 180)
            else:
                fatigue_level = max(0.0, 1.0 - time_since_last_blink / 90)
        else:
            fatigue_level = 0.0
        
        # í˜„ì¬ ìƒíƒœ ì €ì¥
        current_state = {
            'was_blinking': is_blinking,
            'last_blink_frame': frame_count if is_blinking else prev_eye_state.get('last_blink_frame', 0) if prev_eye_state else 0,
            'avg_ear': avg_ear
        }
        
        return blink_count, focus_level, fatigue_level, current_state
    except Exception as e:
        print(f"Eye blink analysis error: {e}")
        return 0, 0.0, 0.0, None


def save_auth_info(username, token):
    """
    ë¡œê·¸ì¸ ì •ë³´ë¥¼ localhost íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        username: ì‚¬ìš©ìëª…
        token: ì¸ì¦ í† í°
    """
    auth_data = {
        "username": username,
        "token": token,
        "saved_at": datetime.now().isoformat()
    }
    try:
        with open(AUTH_INFO_FILE, 'w', encoding='utf-8') as f:
            json.dump(auth_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… ë¡œê·¸ì¸ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {AUTH_INFO_FILE}")
        return True
    except Exception as e:
        print(f"âŒ ë¡œê·¸ì¸ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def load_auth_info():
    """
    ì €ì¥ëœ ë¡œê·¸ì¸ ì •ë³´ë¥¼ localhost íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    
    Returns:
        tuple: (username, token) ë˜ëŠ” (None, None)
    """
    if not os.path.exists(AUTH_INFO_FILE):
        return None, None
    
    try:
        with open(AUTH_INFO_FILE, 'r', encoding='utf-8') as f:
            auth_data = json.load(f)
        return auth_data.get("username"), auth_data.get("token")
    except Exception as e:
        print(f"âŒ ë¡œê·¸ì¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None, None


def register_user(username, email, password, password_confirm):
    """
    íšŒì›ê°€ì… API í˜¸ì¶œ
    
    Args:
        username: ì‚¬ìš©ìëª…
        email: ì´ë©”ì¼
        password: ë¹„ë°€ë²ˆí˜¸
        password_confirm: ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    
    Returns:
        tuple: (success, token) ë˜ëŠ” (False, error_message)
    """
    try:
        data = {
            "username": username,
            "email": email,
            "password": password,
            "password_confirm": password_confirm
        }
        response = requests.post(REGISTER_API_URL, json=data, timeout=10)
        
        if response.status_code == 201:
            result = response.json()
            token = result.get("token")
            if token:
                save_auth_info(username, token)
                return True, token
            return True, None
        else:
            error_msg = response.json()
            if isinstance(error_msg, dict):
                # ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ
                errors = []
                for key, value in error_msg.items():
                    if isinstance(value, list):
                        errors.extend(value)
                    else:
                        errors.append(str(value))
                error_msg = ", ".join(errors)
            return False, error_msg
    except Exception as e:
        return False, str(e)


def login_user(username, password):
    """
    ë¡œê·¸ì¸ API í˜¸ì¶œ ë° í† í° ì €ì¥
    
    Args:
        username: ì‚¬ìš©ìëª…
        password: ë¹„ë°€ë²ˆí˜¸
    
    Returns:
        tuple: (success, token) ë˜ëŠ” (False, error_message)
    """
    try:
        data = {
            "username": username,
            "password": password
        }
        response = requests.post(LOGIN_API_URL, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            token = result.get("token")
            if token:
                save_auth_info(username, token)
                return True, token
            return False, "í† í°ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            error_msg = response.json()
            if isinstance(error_msg, dict):
                non_field_errors = error_msg.get("non_field_errors", [])
                if non_field_errors:
                    error_msg = non_field_errors[0]
                else:
                    error_msg = str(error_msg)
            return False, error_msg
    except Exception as e:
        return False, str(e)


def verify_token(token):
    """
    í† í°ì´ ìœ íš¨í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        token: ì¸ì¦ í† í°
    
    Returns:
        bool: í† í°ì´ ìœ íš¨í•˜ë©´ True
    """
    if not token:
        return False
    
    try:
        headers = {"Authorization": f"Token {token}"}
        # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ í† í° ê²€ì¦ (WellbeingLog ëª©ë¡ ì¡°íšŒ)
        response = requests.get(WELLBEING_API_URL, headers=headers, timeout=5)
        return response.status_code in [200, 201]
    except:
        return False


def get_auth_token():
    """
    ì €ì¥ëœ í† í°ì„ ë¶ˆëŸ¬ì˜¤ê³ , ìœ íš¨ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.
    í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œë˜ì—ˆìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        str: ì¸ì¦ í† í° ë˜ëŠ” None
    """
    username, token = load_auth_info()
    
    if token:
        # í† í° ìœ íš¨ì„± í™•ì¸
        if verify_token(token):
            return token
        else:
            print("âš ï¸ ì €ì¥ëœ í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
            # ë§Œë£Œëœ í† í° ì •ë³´ ì‚­ì œ
            if os.path.exists(AUTH_INFO_FILE):
                os.remove(AUTH_INFO_FILE)
            return None
    else:
        return None


def send_summary_to_server(emotion_window, movement_window, pose_window, head_pose_window, 
                           blink_count_window, focus_window, fatigue_window, frame=None):
    """
    ìµœê·¼ ìœˆë„ìš°ì˜ ê°ì •/í™œë™ì„±/ìì„¸/ê³ ê°œ/ëˆˆ ìƒíƒœë¥¼ ì§‘ê³„í•´ Django ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    ì´ë¯¸ì§€ë„ í•¨ê»˜ ì „ì†¡í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        frame: OpenCV ì´ë¯¸ì§€ (numpy array), Noneì´ë©´ ì´ë¯¸ì§€ ì—†ì´ ì „ì†¡
    """
    if not emotion_window:
        return

    emo_counts = Counter(emotion_window)
    total = sum(emo_counts.values())
    dominant_emotion, dominant_count = emo_counts.most_common(1)[0]
    dominant_ratio = dominant_count / total if total > 0 else 0.0

    avg_movement = float(np.mean(movement_window)) if movement_window else 0.0
    
    # ìì„¸ ì§‘ê³„ (ê°€ì¥ ë¹ˆë„ ë†’ì€ ìì„¸)
    dominant_pose = None
    if pose_window:
        pose_counts = Counter(pose_window)
        dominant_pose = pose_counts.most_common(1)[0][0] if pose_counts else None
    
    # Head Pose í‰ê·  ê³„ì‚°
    avg_head_pose = None
    if head_pose_window:
        valid_poses = [p for p in head_pose_window if p is not None]
        if valid_poses:
            avg_pitch = np.mean([p.get('pitch', 0) for p in valid_poses])
            avg_yaw = np.mean([p.get('yaw', 0) for p in valid_poses])
            avg_roll = np.mean([p.get('roll', 0) for p in valid_poses])
            avg_head_pose = {
                "pitch": float(avg_pitch),
                "yaw": float(avg_yaw),
                "roll": float(avg_roll)
            }
    
    # ëˆˆ ê¹œë¹¡ì„ ì´í•©
    total_blinks = sum(blink_count_window) if blink_count_window else 0
    
    # ì§‘ì¤‘ë„ì™€ í”¼ë¡œë„ í‰ê· 
    avg_focus = float(np.mean(focus_window)) if focus_window else 0.0
    avg_fatigue = float(np.mean(fatigue_window)) if fatigue_window else 0.0

    # ì €ì¥ëœ í† í° ì‚¬ìš©
    token = get_auth_token()
    headers = {}
    if token:
        headers["Authorization"] = f"Token {token}"
    else:
        print("âš ï¸ ì¸ì¦ í† í°ì´ ì—†ì–´ ìš”ì²­ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    try:
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ multipart/form-dataë¡œ ì „ì†¡
        if frame is not None:
            # ì´ë¯¸ì§€ë¥¼ JPEG í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©
            _, img_encoded = cv2.imencode('.jpg', frame)
            img_bytes = img_encoded.tobytes()
            
            # multipart/form-dataë¡œ ì „ì†¡
            files = {
                'image': ('wellbeing_image.jpg', BytesIO(img_bytes), 'image/jpeg')
            }
            
            data = {
                "dominant_emotion": dominant_emotion,
                "dominant_emotion_ratio": str(dominant_ratio),
                "emotion_counts": json.dumps(dict(emo_counts), ensure_ascii=False),
                "avg_movement": str(avg_movement),
                "pose": dominant_pose or "",
                "head_pose": json.dumps(avg_head_pose, ensure_ascii=False) if avg_head_pose else "{}",
                "eye_blink_count": str(total_blinks),
                "focus_level": str(avg_focus),
                "fatigue_level": str(avg_fatigue),
            }
            
            resp = requests.post(
                WELLBEING_API_URL, data=data, files=files, headers=headers, timeout=10
            )
        else:
            # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ JSONìœ¼ë¡œ ì „ì†¡ (ê¸°ì¡´ ë°©ì‹)
            payload = {
                "dominant_emotion": dominant_emotion,
                "dominant_emotion_ratio": dominant_ratio,
                "emotion_counts": dict(emo_counts),
                "avg_movement": avg_movement,
                "pose": dominant_pose,
                "head_pose": avg_head_pose,
                "eye_blink_count": total_blinks,
                "focus_level": avg_focus,
                "fatigue_level": avg_fatigue,
                "timestamp": time.time(),
            }
            headers["Content-Type"] = "application/json"
            resp = requests.post(
                WELLBEING_API_URL, json=payload, headers=headers, timeout=5
            )
        
        if resp.status_code in [200, 201]:
            print("âœ… WellbeingLog ì „ì†¡ ì„±ê³µ (ì´ë¯¸ì§€ í¬í•¨)" if frame is not None else "âœ… WellbeingLog ì „ì†¡ ì„±ê³µ")
        else:
            print(f"âš ï¸ WellbeingLog ì „ì†¡ ì‹¤íŒ¨ ({resp.status_code}): {resp.text[:200]}")
    except Exception as e:
        print("ì„œë²„ ì „ì†¡ ì˜¤ë¥˜:", e)


def send_post_to_server(frame, detected_objects, title=None, text=None):
    """
    ê²€ì¶œëœ ê°ì²´ ì •ë³´ë¥¼ í¬í•¨í•œ ì´ë¯¸ì§€ë¥¼ Post APIë¡œ Django ì„œë²„ì— ê²Œì‹œí•©ë‹ˆë‹¤.
    
    Args:
        frame: OpenCV ì´ë¯¸ì§€ (numpy array)
        detected_objects: ê²€ì¶œëœ ê°ì²´ ë”•ì…”ë„ˆë¦¬ {object_name: count}
        title: í¬ìŠ¤íŠ¸ ì œëª© (Noneì´ë©´ ìë™ ìƒì„±)
        text: í¬ìŠ¤íŠ¸ ë‚´ìš© (Noneì´ë©´ ìë™ ìƒì„±)
    """
    if not ENABLE_POST_API:
        return
    
    if not detected_objects:
        return
    
    try:
        # ì œëª©ê³¼ ë‚´ìš© ìë™ ìƒì„±
        if title is None:
            obj_list = ", ".join([f"{name}({count})" for name, count in list(detected_objects.items())[:5]])
            title = f"Detected Objects: {obj_list}"
        
        if text is None:
            text = f"Detected objects at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            text += "Objects detected:\n"
            for obj_name, count in sorted(detected_objects.items(), key=lambda x: x[1], reverse=True):
                text += f"- {obj_name}: {count}\n"
        
        # ì´ë¯¸ì§€ë¥¼ JPEG í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©
        _, img_encoded = cv2.imencode('.jpg', frame)
        img_bytes = img_encoded.tobytes()
        
        # íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ multipart/form-data ì¤€ë¹„
        files = {
            'image': ('detection.jpg', BytesIO(img_bytes), 'image/jpeg')
        }
        
        data = {
            'title': title,
            'text': text,
        }
        
        headers = {}
        
        # ì €ì¥ëœ í† í° ì‚¬ìš©
        token = get_auth_token()
        if token:
            headers['Authorization'] = f"Token {token}"
        else:
            print("âš ï¸ ì¸ì¦ í† í°ì´ ì—†ì–´ ìš”ì²­ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # Post APIë¡œ ì „ì†¡
        resp = requests.post(
            POST_API_URL,
            data=data,
            files=files,
            headers=headers,
            timeout=10
        )
        
        if resp.status_code in [200, 201]:
            print(f"âœ… Post ê²Œì‹œ ì„±ê³µ: {title}")
        else:
            print(f"âš ï¸ Post ê²Œì‹œ ì‹¤íŒ¨ ({resp.status_code}): {resp.text[:200]}")
            
    except Exception as e:
        print(f"âŒ Post ê²Œì‹œ ì˜¤ë¥˜: {e}")


def main():
    global MEDIAPIPE_AVAILABLE
    # ë¡œê·¸ì¸ í™•ì¸
    print("="*60)
    print("  ì‚¬ìš©ì ì¸ì¦ í™•ì¸")
    print("="*60)
    
    username, token = load_auth_info()
    
    # í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš°
    if not token or not verify_token(token):
        if token:
            print("âš ï¸ ì €ì¥ëœ í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            if os.path.exists(AUTH_INFO_FILE):
                os.remove(AUTH_INFO_FILE)
        
        print("\në¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("\n[1] íšŒì›ê°€ì…")
        print("[2] ë¡œê·¸ì¸")
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
        
        if choice == "1":
            print("\n=== íšŒì›ê°€ì… ===")
            username = input("ì‚¬ìš©ìëª…: ").strip()
            email = input("ì´ë©”ì¼ (ì„ íƒì‚¬í•­): ").strip() or ""
            password = input("ë¹„ë°€ë²ˆí˜¸: ").strip()
            password_confirm = input("ë¹„ë°€ë²ˆí˜¸ í™•ì¸: ").strip()
            
            success, result = register_user(username, email, password, password_confirm)
            if success:
                print(f"âœ… íšŒì›ê°€ì… ì„±ê³µ! í† í°ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                token = result
            else:
                print(f"âŒ íšŒì›ê°€ì… ì‹¤íŒ¨: {result}")
                return
        
        elif choice == "2":
            print("\n=== ë¡œê·¸ì¸ ===")
            username = input("ì‚¬ìš©ìëª…: ").strip()
            password = input("ë¹„ë°€ë²ˆí˜¸: ").strip()
            
            success, result = login_user(username, password)
            if success:
                print(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ! í† í°ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                token = result
            else:
                print(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {result}")
                return
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return
    else:
        print(f"âœ… ì €ì¥ëœ ë¡œê·¸ì¸ ì •ë³´ ì‚¬ìš©: {username}")
    
    print("="*60)
    print()
    
    yolo = load_yolo_model()
    
    # MediaPipe ì´ˆê¸°í™” (tasks API)
    pose_landmarker = None
    face_landmarker = None
    
    if MEDIAPIPE_AVAILABLE:
        try:
            print("MediaPipe ëª¨ë¸ ë¡œë”© ì¤‘...")
            from mediapipe.tasks.python import vision
            import urllib.request
            import tempfile
            
            # ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
            model_dir = os.path.join(os.path.dirname(__file__), "mediapipe_models")
            os.makedirs(model_dir, exist_ok=True)
            
            # Pose Landmarker ëª¨ë¸ íŒŒì¼
            pose_model_path = os.path.join(model_dir, "pose_landmarker.task")
            if not os.path.exists(pose_model_path):
                print("   Pose ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                pose_model_url = "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task"
                urllib.request.urlretrieve(pose_model_url, pose_model_path)
            
            # Face Landmarker ëª¨ë¸ íŒŒì¼
            face_model_path = os.path.join(model_dir, "face_landmarker.task")
            if not os.path.exists(face_model_path):
                print("   Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                face_model_url = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
                urllib.request.urlretrieve(face_model_url, face_model_path)
            
            # Pose Landmarker ì´ˆê¸°í™”
            base_options = python.BaseOptions(model_asset_path=pose_model_path)
            options = vision.PoseLandmarkerOptions(
                base_options=base_options,
                output_segmentation_masks=False,
                min_pose_detection_confidence=0.5,
                min_pose_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            pose_landmarker = vision.PoseLandmarker.create_from_options(options)
            
            # Face Landmarker ì´ˆê¸°í™”
            face_base_options = python.BaseOptions(model_asset_path=face_model_path)
            face_options = vision.FaceLandmarkerOptions(
                base_options=face_base_options,
                output_face_blendshapes=False,
                output_facial_transformation_matrixes=False,
                num_faces=1,
                min_face_detection_confidence=0.5,
                min_face_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            face_landmarker = vision.FaceLandmarker.create_from_options(face_options)
            
            print("âœ… MediaPipe ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"   ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
            MEDIAPIPE_AVAILABLE = False
            pose_landmarker = None
            face_landmarker = None
    else:
        print("âš ï¸ MediaPipeë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìì„¸/Head Pose/ëˆˆ ë¶„ì„ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    print("ì›¹ìº  ì—´ê¸°...")
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    emotion_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    movement_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    pose_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    head_pose_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    blink_count_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    focus_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    fatigue_window = deque(maxlen=WINDOW_SECONDS * FRAME_FPS_ASSUMPTION)
    
    prev_person_boxes = []
    prev_eye_state = None
    frame_count = 0
    last_summary_time = time.time()
    last_post_time = time.time()
    prev_detected_objects = set()  # ì´ì „ í”„ë ˆì„ì—ì„œ ê²€ì¶œëœ ê°ì²´ ì¶”ì 

    while True:
        ret, frame = cap.read()
        if not ret:
            print("í”„ë ˆì„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        results = yolo(frame)
        det = results.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2, conf, class]
        
        # COCO í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        class_names = results.names  # {0: 'person', 1: 'bicycle', ...}

        person_boxes = []
        detected_objects = {}  # ê²€ì¶œëœ ê°ì²´ ì¹´ìš´íŠ¸

        for *xyxy, conf, cls in det:
            x1, y1, x2, y2 = map(int, xyxy)
            cls_id = int(cls)
            cls_name = class_names.get(cls_id, f"class_{cls_id}")
            conf_score = float(conf)
            
            # ê²€ì¶œëœ ê°ì²´ ì¹´ìš´íŠ¸
            detected_objects[cls_name] = detected_objects.get(cls_name, 0) + 1
            
            # ì‚¬ëŒ(class 0)ì¸ ê²½ìš°ì—ë§Œ ê°ì • ë¶„ì„ ìˆ˜í–‰
            if cls_id == 0:  # person
                person_boxes.append((x1, y1, x2, y2))

                # ìƒë°˜ì‹  ìƒë‹¨ ë¶€ë¶„ì„ ì–¼êµ´ë¡œ ê°„ì£¼í•œ ê°„ë‹¨í•œ crop
                h = y2 - y1
                face_y2 = y1 + int(h * 0.6)
                face_img = frame[y1:face_y2, x1:x2]
                if face_img.size > 0:
                    emo = analyze_emotion(face_img)
                    if emo:
                        emotion_window.append(emo)
                    
                    # ì‚¬ëŒ ë°•ìŠ¤ëŠ” ì´ˆë¡ìƒ‰, ê°ì • í‘œì‹œ
                    color = (0, 255, 0)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    label = f"{cls_name} {conf_score:.2f}"
                    if emo:
                        label += f" [{emo}]"
                    cv2.putText(
                        frame,
                        label,
                        (x1, max(y1 - 5, 0)),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        color,
                        2,
                    )
            else:
                # ì‚¬ëŒì´ ì•„ë‹Œ ë‹¤ë¥¸ ê°ì²´ë“¤ì€ íŒŒë€ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                color = (255, 0, 0)
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                label = f"{cls_name} {conf_score:.2f}"
                cv2.putText(
                    frame,
                    label,
                    (x1, max(y1 - 5, 0)),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    color,
                    1,
                )

        movement = estimate_movement(prev_person_boxes, person_boxes)
        movement_window.append(movement)
        prev_person_boxes = person_boxes
        
        # MediaPipe ë¶„ì„ (ì‚¬ëŒì´ ê²€ì¶œëœ ê²½ìš°)
        current_pose = None
        current_head_pose = None
        blink_count = 0
        focus_level = 0.0
        fatigue_level = 0.0
        
        if person_boxes and MEDIAPIPE_AVAILABLE and pose_landmarker and face_landmarker:
            try:
                # RGBë¡œ ë³€í™˜ (MediaPipeëŠ” RGBë¥¼ ì‚¬ìš©)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w = frame.shape[:2]
                
                # MediaPipe Image ìƒì„±
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
                
                # Pose ì¶”ì •
                pose_detection_result = pose_landmarker.detect(mp_image)
                if pose_detection_result.pose_landmarks and len(pose_detection_result.pose_landmarks) > 0:
                    # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ ëœë“œë§ˆí¬ ì‚¬ìš©
                    pose_landmarks = pose_detection_result.pose_landmarks[0]
                    current_pose = estimate_pose(pose_landmarks, h, w)
                    if current_pose:
                        pose_window.append(current_pose)
                
                # Face Mesh (Head Pose & Eye Blink)
                face_detection_result = face_landmarker.detect(mp_image)
                if face_detection_result.face_landmarks and len(face_detection_result.face_landmarks) > 0:
                    # ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ëœë“œë§ˆí¬ ì‚¬ìš©
                    face_landmarks = face_detection_result.face_landmarks[0]
                    
                    # Head Pose ì¶”ì •
                    current_head_pose = estimate_head_pose(face_landmarks, w, h)
                    if current_head_pose:
                        head_pose_window.append(current_head_pose)
                    
                    # ëˆˆ ê¹œë¹¡ì„ ë° ì§‘ì¤‘ë„/í”¼ë¡œë„ ë¶„ì„
                    blink_count, focus_level, fatigue_level, new_eye_state = analyze_eye_blink(
                        face_landmarks, prev_eye_state, frame_count
                    )
                    if new_eye_state:
                        prev_eye_state = new_eye_state
                    if blink_count > 0:
                        blink_count_window.append(blink_count)
                    focus_window.append(focus_level)
                    fatigue_window.append(fatigue_level)
            except Exception as e:
                print(f"MediaPipe ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        frame_count += 1

        # í™”ë©´ì— ì •ë³´ í‘œì‹œ
        y_offset = 30
        cv2.putText(
            frame,
            f"movement: {movement:.1f}",
            (10, y_offset),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (0, 255, 255),
            2,
        )
        
        # ìì„¸ ì •ë³´ í‘œì‹œ
        if current_pose:
            y_offset += 30
            cv2.putText(
                frame,
                f"pose: {current_pose}",
                (10, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 0),
                2,
            )
        
        # ì§‘ì¤‘ë„/í”¼ë¡œë„ í‘œì‹œ
        if focus_level > 0 or fatigue_level > 0:
            y_offset += 30
            cv2.putText(
                frame,
                f"focus: {focus_level:.2f} | fatigue: {fatigue_level:.2f}",
                (10, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 200, 0),
                2,
            )
        
        # ê²€ì¶œëœ ê°ì²´ ëª©ë¡ í‘œì‹œ (ìµœëŒ€ 5ê°œ)
        y_offset += 30
        detected_list = list(detected_objects.items())[:5]
        for i, (obj_name, count) in enumerate(detected_list):
            cv2.putText(
                frame,
                f"{obj_name}: {count}",
                (10, y_offset + i * 25),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (255, 255, 255),
                1,
            )

        cv2.imshow("Wellbeing Analyzer", frame)

        # ESCí‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == 27:
            break

        now = time.time()
        
        # WellbeingLog ì „ì†¡ (ì£¼ê¸°ì )
        if now - last_summary_time > SUMMARY_INTERVAL:
            send_summary_to_server(
                emotion_window, movement_window, pose_window, head_pose_window,
                blink_count_window, focus_window, fatigue_window, frame=frame.copy()
            )
            last_summary_time = now
        
        # Post ê²Œì‹œ ì²˜ë¦¬
        if ENABLE_POST_API:
            current_objects = set(detected_objects.keys())
            
            # ìƒˆ ê°ì²´ê°€ ê²€ì¶œë˜ì—ˆì„ ë•Œ ì¦‰ì‹œ ê²Œì‹œ
            if POST_ON_NEW_OBJECT:
                new_objects = current_objects - prev_detected_objects
                if new_objects:
                    print(f"ğŸ†• ìƒˆ ê°ì²´ ê²€ì¶œ: {', '.join(new_objects)}")
                    send_post_to_server(frame.copy(), detected_objects)
                    prev_detected_objects = current_objects.copy()
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ê²Œì‹œ (ê²€ì¶œëœ ê°ì²´ê°€ ìˆì„ ë•Œë§Œ)
            if now - last_post_time > POST_INTERVAL and detected_objects:
                send_post_to_server(frame.copy(), detected_objects)
                last_post_time = now
            
            # prev_detected_objects ì—…ë°ì´íŠ¸
            prev_detected_objects = current_objects.copy()

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()


